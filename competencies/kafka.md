# Competency - Kafka

Kafka combines three key capabilities so you can implement your use cases for event streaming end-to-end with a single battle-tested solution:

1. To publish (write) and subscribe to (read) streams of events, including continuous import/export of your data from other systems.
2. To store streams of events durably and reliably for as long as you want.
3. To process streams of events as they occur or retrospectively.

And all this functionality is provided in a distributed, highly scalable, elastic, fault-tolerant, and secure manner. Kafka can be deployed on bare-metal hardware, virtual machines, and containers, and on-premises as well as in the cloud. You can choose between self-managing your Kafka environments and using fully managed services offered by a variety of vendors.

_Source: https://kafka.apache.org/_

## How do you prove it?

* You know what Kafka is and when it can be used.

* You know the advantages and disadvantages of using Kafka.

* You know what producers and consumers are.

* You know the elements of Kafka message.

* You know what topics, partitions and brokers are.

* You know what ZooKeeper does.

* You know what leaders and followers are.

* You can use at least one Kafka client.

* You know what consumer group id is used for and how it influences message delivery

* You understand implications of using keys in messages

* You understand what request-reply is and know how/when to use it

* You know how to use kafka command line tools to consume, produce messages, list topics etc

* You understand how kafka authentication works and how to configure it

## How do you improve it?

Read the official documentation: https://kafka.apache.org/documentation/

Do a tutorial: https://data-flair.training/blogs/apache-kafka-tutorial/