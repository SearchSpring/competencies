# Competency - Kafka

Kafka combines three key capabilities so you can implement your use cases for event streaming end-to-end with a single battle-tested solution:

1. To publish (write) and subscribe to (read) streams of events, including continuous import/export of your data from other systems.
2. To store streams of events durably and reliably for as long as you want.
3. To process streams of events as they occur or retrospectively.

And all this functionality is provided in a distributed, highly scalable, elastic, fault-tolerant, and secure manner. Kafka can be deployed on bare-metal hardware, virtual machines, and containers, and on-premises as well as in the cloud. You can choose between self-managing your Kafka environments and using fully managed services offered by a variety of vendors.

_Source: https://kafka.apache.org/_

## How do you prove it?

You know what Kafka is and when it can be used.

You know the advantages and disadvantages of using Kafka.

You know what producers and consumers are.

You know the elements of Kafka message.

You know what topics, partitions and brokers are.

You know what ZooKeeper does.

You know what leaders and followers are.

You can use at least one Kafka client.

## How do you improve it?

Read the official documentation: https://kafka.apache.org/documentation/

Do a tutorial: https://data-flair.training/blogs/apache-kafka-tutorial/